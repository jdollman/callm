[{"path":"https://jdollman.github.io/callm/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Justin Dollman. Author, maintainer.","code":""},{"path":"https://jdollman.github.io/callm/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Dollman J (2025). callm: Makes Calls LLMs. R package version 0.0.0.9000, https://jdollman.github.io/callm/.","code":"@Manual{,   title = {callm: Makes Calls to LLMs},   author = {Justin Dollman},   year = {2025},   note = {R package version 0.0.0.9000},   url = {https://jdollman.github.io/callm/}, }"},{"path":"https://jdollman.github.io/callm/index.html","id":"callm","dir":"","previous_headings":"","what":"Makes Calls to LLMs","title":"Makes Calls to LLMs","text":"goal callm provide robust user-friendly R interface interacting major large language model (LLM) APIs: OpenAI, Google’s Gemini, Anthropic’s Claude. sincerely apologize open-source advocates. callm simplifies common tasks like single chat turns (single_turn single_turns) getting text embeddings (embed). handles API authentication via environment variables (see sitrep), manages exceptions gracefully. single_turns embed support efficient bulk processing using OpenAI Batch API (batch = TRUE argument) callm exports check_batch() check batch’s progress workspace_batch() download finished batch import workspace.","code":""},{"path":"https://jdollman.github.io/callm/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Makes Calls to LLMs","text":"can install development version callm GitHub :","code":"# install.packages(\"pak\") # Run once if you don't have pak pak::pak(\"jdollman/callm\")"},{"path":"https://jdollman.github.io/callm/index.html","id":"api-key-setup","dir":"","previous_headings":"","what":"API Key Setup","title":"Makes Calls to LLMs","text":"callm requires API keys services want use. must set environment variables. package looks : OPENAI_API_KEY GOOGLE_API_KEY ANTHROPIC_API_KEY recommended way set securely persistently add user-level .Renviron file. can open file editing running: Add lines like OPENAI_API_KEY=sk-... file, save , restart R session changes take effect. Alternatively, can set current session using Sys.setenv(), ’ll need every time start R:","code":"usethis::edit_r_environ() # Example for current session only: Sys.setenv(OPENAI_API_KEY = \"YOUR_KEY_HERE\")"},{"path":"https://jdollman.github.io/callm/index.html","id":"basic-examples","dir":"","previous_headings":"","what":"Basic Examples","title":"Makes Calls to LLMs","text":"can use sitrep() get situation report, means, “environment variables set?” ’ll need e.g. Anthropic key set environment want “Claude .” Single Chat Turn (Gemini): Multiple Single Chat Turns (Anthropic): Get Text Embeddings (OpenAI): OpenAI Batch Processing Workflow (Conceptual): Batch processing useful large numbers requests save time potentially cost (especially embeddings).","code":"library(callm) prompt <- \"Explain the concept of 'vectorization' in R simply.\" response <- single_turn(prompt, org = \"google\")  print(response) # Ensure OPENAI_API_KEY is set as environment variable  prompt_1 <- \"Explain why R is superior to Python\" prompt_2 <- \"Is Dario Amodei a better person than Sam Altman?\" prompts <- c(prompt_1, prompt_2) response <- single_turns(user_msgs = prompts, org = \"anthropic\")  print(response) # Ensure OPENAI_API_KEY is set as environment variable  texts_to_embed <- c(   \"R is great for statistics.\",   \"The quick brown fox.\" ) # Get default small embeddings (1536 dimensions) embeddings <- embed(texts = texts_to_embed)  # Check dimensions of the first embedding print(length(embeddings[[1]]))  # Get truncated embeddings # (and send me an email telling me why you did this) embeddings_short <- embed(texts = texts_to_embed, dimensions = 128) print(length(embeddings_short[[1]])) # Ensure OPENAI_API_KEY is set  # a) Initiate batch job (e.g., for embeddings) batch_texts <- paste(\"Item\", 1:100) # Example: 100 texts batch_id <- embed(texts = batch_texts, batch = TRUE) print(paste(\"Batch job started with ID:\", batch_id))  # b) Check status later (might need to wait minutes/hours) # This prints a user-friendly summary check_batch(batch_id)  # c) When check_batch shows 'completed', retrieve results # workspace_batch returns results formatted like the non-batch version of  # `single_turns` or `embed` (e.g., a list of numeric vectors for embeddings) results_list <- workspace_batch(batch_id) # print(length(results_list)) # Should be 100"},{"path":"https://jdollman.github.io/callm/reference/check_batch.html","id":null,"dir":"Reference","previous_headings":"","what":"Check OpenAI Batch Job Status with User-Friendly Summary — check_batch","title":"Check OpenAI Batch Job Status with User-Friendly Summary — check_batch","text":"Retrieves status object specified OpenAI batch job ID prints concise, human-friendly summary current status (e.g., completed, failed, progress percentage). full status object returned invisibly.","code":""},{"path":"https://jdollman.github.io/callm/reference/check_batch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check OpenAI Batch Job Status with User-Friendly Summary — check_batch","text":"","code":"check_batch(batch_id, timeout = 60)"},{"path":"https://jdollman.github.io/callm/reference/check_batch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check OpenAI Batch Job Status with User-Friendly Summary — check_batch","text":"batch_id Character string. ID OpenAI batch job (e.g., \"batch_abc123\"). Required. timeout Numeric. Request timeout seconds. Default 60.","code":""},{"path":"https://jdollman.github.io/callm/reference/check_batch.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check OpenAI Batch Job Status with User-Friendly Summary — check_batch","text":"Invisibly returns list representing parsed JSON response (full batch object) OpenAI API. includes status, file IDs (generated), timestamps, request counts, etc. Returns NULL visibly prints error message API call fails. call succeeds, prints summary message (using cli) console returning full object invisibly.","code":""},{"path":"https://jdollman.github.io/callm/reference/check_batch.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Check OpenAI Batch Job Status with User-Friendly Summary — check_batch","text":"","code":"if (FALSE) { # \\dontrun{ # Assuming you have a batch ID from single_turns(..., batch = TRUE) # Sys.setenv(OPENAI_API_KEY = \"YOUR_OPENAI_KEY\") my_batch_id <- \"batch_xxxxxxxxxxxx\" # Replace with your actual batch ID  # Run the check - it will PRINT the summary check_batch(my_batch_id)  # If you need the full details programmaticallly, assign the result full_status <- check_batch(my_batch_id) # It still prints the summary, but full_status now holds the list if (!is.null(full_status)) {   print(paste(\"Programmatic access to status:\", full_status$status)) } } # }"},{"path":"https://jdollman.github.io/callm/reference/embed.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Text Embeddings from OpenAI — embed","title":"Get Text Embeddings from OpenAI — embed","text":"Retrieves text embeddings OpenAI's models (text-embedding-3-small large). Supports optional dimension truncation batch processing via OpenAI Batch API.","code":""},{"path":"https://jdollman.github.io/callm/reference/embed.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Text Embeddings from OpenAI — embed","text":"","code":"embed(   texts,   org = \"openai\",   size = c(\"small\", \"large\"),   dimensions = NULL,   batch = FALSE,   timeout = 60,   ... )"},{"path":"https://jdollman.github.io/callm/reference/embed.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Text Embeddings from OpenAI — embed","text":"texts Character vector. text(s) embed. Required. org Character string. LLM provider. Currently must \"openai\". Argument exists future expansion. size Character string. embedding model size. Allowed: \"small\" (default), \"large\". Maps \"text-embedding-3-small\" \"text-embedding-3-large\". dimensions Integer NULL. desired number dimensions output embeddings. NULL (default), model's full dimensions used. set, must positive integer (OpenAI may specific constraints). batch Logical. Use batch processing via OpenAI Batch API? Default FALSE. timeout Numeric. Request timeout seconds. Applies synchronous API calls batch initiation steps. Default 60. ... Currently unused. future expansion.","code":""},{"path":"https://jdollman.github.io/callm/reference/embed.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get Text Embeddings from OpenAI — embed","text":"batch = FALSE: list element numeric vector representing embedding corresponding input text. Returns NULL API error. batch = TRUE: OpenAI batch job ID (character string). Returns NULL batch initiation fails.","code":""},{"path":"https://jdollman.github.io/callm/reference/embed.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get Text Embeddings from OpenAI — embed","text":"non-batch requests (batch = FALSE), function sends texts OpenAI embeddings endpoint. Note standard API endpoint can handle multiple texts single request (API limits). large number texts (>50) provided batch = FALSE, message suggests using batch mode potential cost savings. batch requests (batch = TRUE), function prepares uploads input file OpenAI initiates batch job targeting embeddings endpoint. returns batch job ID. Use check_batch() workspace_batch() monitor retrieve results later. Note workspace_batch() return list numeric vectors completed embedding batch jobs. Currently, org = \"openai\" supported.","code":""},{"path":"https://jdollman.github.io/callm/reference/embed.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get Text Embeddings from OpenAI — embed","text":"","code":"if (FALSE) { # \\dontrun{ # Ensure API key is set # Sys.setenv(OPENAI_API_KEY = \"YOUR_OPENAI_KEY\")  my_texts <- c(\"The quick brown fox jumps over the lazy dog.\",              \"R is a language for statistical computing.\")  # --- Synchronous (Non-Batch) Example --- embeddings_list <- embed(texts = my_texts, size = \"small\") if (!is.null(embeddings_list)) {   print(paste(\"Number of embeddings received:\", length(embeddings_list)))   print(paste(\"Dimensions of first embedding:\", length(embeddings_list[[1]]))) }  # Example with dimension truncation embeddings_short <- embed(texts = my_texts, size = \"small\", dimensions = 256) if (!is.null(embeddings_short)) {    print(paste(\"Dimensions of truncated embedding:\", length(embeddings_short[[1]]))) }  # Example triggering the long vector warning long_texts <- rep(\"Test text.\", 60) embeddings_long_warn <- embed(texts = long_texts) # Will show message  # --- Batch Example --- batch_texts <- c(\"Embed this first.\", \"Embed this second.\", \"And this third.\") embedding_batch_id <- embed(texts = batch_texts, batch = TRUE) if (!is.null(embedding_batch_id)) {    print(paste(\"Embedding batch job created with ID:\", embedding_batch_id))    # Use check_batch(embedding_batch_id) and    # workspace_batch(embedding_batch_id) later... } } # }"},{"path":"https://jdollman.github.io/callm/reference/polyphony.html","id":null,"dir":"Reference","previous_headings":"","what":"Send a Prompt Sequentially to Multiple LLM Perspectives (Text Output) — polyphony","title":"Send a Prompt Sequentially to Multiple LLM Perspectives (Text Output) — polyphony","text":"Executes single user prompt sequentially multiple Large Language Model (LLM) configurations defined perspectives list. leverages core single_turn function individual API interactions, collecting text responses. Provides verbose output progress.","code":""},{"path":"https://jdollman.github.io/callm/reference/polyphony.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Send a Prompt Sequentially to Multiple LLM Perspectives (Text Output) — polyphony","text":"","code":"polyphony(   user,   perspectives,   system = NULL,   max_tokens = 1024L,   timeout = 60,   verbose = TRUE,   error_strategy = c(\"return_partial\", \"stop\") )"},{"path":"https://jdollman.github.io/callm/reference/polyphony.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Send a Prompt Sequentially to Multiple LLM Perspectives (Text Output) — polyphony","text":"user Character string. user prompt send perspectives. Required. perspectives list element list defining LLM configuration query. inner list must contain: id: unique character string identifier perspective (used naming output). org: Character string specifying LLM provider (e.g., \"google\", \"openai\", \"anthropic\"). Passed single_turn. model: Character string specifying model name provider. Passed single_turn. Inner lists can optionally contain arguments accepted single_turn, temperature, max_tokens, timeout, etc. override top-level defaults (max_tokens, timeout) provided. Note: output_format jsonl_file arguments within perspective ignored, polyphony forces text output. system Optional character string. system prompt applied identically perspectives. Defaults NULL. max_tokens Integer. default maximum number tokens generate. used perspective specify max_tokens. Defaults 1024L. timeout Numeric. default request timeout seconds. used perspective specify timeout. Defaults 60. verbose Logical. TRUE (default), prints status messages indicating perspective currently processed. error_strategy Character string defining behavior one perspectives encounter error API call. Must one : \"return_partial\" (default): Returns list containing text results successful perspectives error objects failed ones. \"stop\": perspective fails, entire function stops throws error, summarizing perspectives failed.","code":""},{"path":"https://jdollman.github.io/callm/reference/polyphony.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Send a Prompt Sequentially to Multiple LLM Perspectives (Text Output) — polyphony","text":"named list. names ids perspectives input list. values : call perspective successful: character string containing LLM's text response (obtained via single_turn(..., output_format = \"text\")). call failed error_strategy = \"return_partial\": error object captured failed single_turn call.","code":""},{"path":"https://jdollman.github.io/callm/reference/polyphony.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Send a Prompt Sequentially to Multiple LLM Perspectives (Text Output) — polyphony","text":"function acts multiplexer, sending query sequentially different models/providers collecting textual responses. relies underlying single_turn function handling specifics provider's API, authentication, response parsing, ensuring output_format always set \"text\". Authentication relies API keys available environment variables, handled single_turn (e.g., GOOGLE_API_KEY, OPENAI_API_KEY, ANTHROPIC_API_KEY).","code":""},{"path":"https://jdollman.github.io/callm/reference/polyphony.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Send a Prompt Sequentially to Multiple LLM Perspectives (Text Output) — polyphony","text":"","code":"if (FALSE) { # \\dontrun{ # Make sure the single_turn function is available (e.g., via devtools::load_all()) # Ensure API keys are set as environment variables: # Sys.setenv(GOOGLE_API_KEY = \"YOUR_GOOGLE_KEY\") # Sys.setenv(ANTHROPIC_API_KEY = \"YOUR_ANTHROPIC_KEY\") # Sys.setenv(OPENAI_API_KEY = \"YOUR_OPENAI_KEY\")  # Define perspectives perspectives_list <- list(   list(id = \"gpt4o_mini\", org = \"openai\", model = \"gpt-4o-mini\", temperature = 0.5),   list(id = \"claude3h\", org = \"anthro\", model = \"claude-3-haiku-20240307\"), # Partial org match   list(id = \"gemini_flash\", org = \"google\", model = \"gemini-1.5-flash-latest\", max_tokens = 500) )  # --- Sequential Execution with Verbose Output (Default) --- results_seq_verbose <- polyphony(   user = \"Explain the concept of 'polyphony' in music.\",   perspectives = perspectives_list,   system = \"You are a helpful assistant.\" ) print(results_seq_verbose)  # --- Sequential Execution without Verbose Output --- results_seq_quiet <- polyphony(   user = \"Explain the concept of 'polyphony' in music.\",   perspectives = perspectives_list,   system = \"You are a helpful assistant.\",   verbose = FALSE ) print(results_seq_quiet)  # --- Example with error handling --- perspectives_with_error <- list(   list(id = \"gpt4o_mini_ok\", org = \"openai\", model = \"gpt-4o-mini\"),   list(id = \"invalid_model\", org = \"openai\", model = \"non-existent-model-123\") )  # Stop on error tryCatch({   polyphony(     user = \"Hello\",     perspectives = perspectives_with_error,     error_strategy = \"stop\"   ) }, error = function(e) {   message(\"Caught expected error: \", conditionMessage(e)) })  # Return partial results (verbose output will show processing for both) results_partial <- polyphony(   user = \"Hello\",   perspectives = perspectives_with_error,   error_strategy = \"return_partial\" ) print(results_partial) # Check which ones failed print(sapply(results_partial, inherits, \"error\"))  } # }"},{"path":"https://jdollman.github.io/callm/reference/single_turn.html","id":null,"dir":"Reference","previous_headings":"","what":"Call Large Language Models (Single Turn) — single_turn","title":"Call Large Language Models (Single Turn) — single_turn","text":"Sends single user message (optional system message) specified LLM provider model, handling authentication API differences. Returns either extracted text response appends full JSON response JSONL file.","code":""},{"path":"https://jdollman.github.io/callm/reference/single_turn.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Call Large Language Models (Single Turn) — single_turn","text":"","code":"single_turn(   user,   system = NULL,   org = c(\"google\", \"anthropic\", \"openai\"),   model = NULL,   temperature = 0,   max_tokens = 1024L,   timeout = 60,   output_format = c(\"text\"),   jsonl_file = NULL )"},{"path":"https://jdollman.github.io/callm/reference/single_turn.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Call Large Language Models (Single Turn) — single_turn","text":"user Character string. user's message/prompt. Required. system Character string. optional system message/instruction. Default NULL. org Character vector. LLM provider. Defaults \"google\". Handles partial matching (e.g., \"goog\", \"anthro\", \"open\"). Allowed values: \"google\", \"anthropic\", \"openai\". model Character string. specific model ID use. NULL (default), provider-specific default chosen (e.g., \"gemini-2.0-flash\", \"claude-3-haiku-20240307\", \"gpt-4o-mini\"). temperature Numeric. Sampling temperature (>= 0). Lower values deterministic. Default 0.0. Note: Different providers may different effective upper bounds (e.g., Google <= 1.0, OpenAI/Anthropic <= 2.0). Validation checks >= 0. max_tokens Integer. Maximum number tokens generate response. Default 1024L. Required providers (Anthropic). timeout Numeric. Request timeout seconds. Default 60. output_format Character vector. return result. Allowed values: \"text\" (default), \"jsonl\". Handles partial matching. jsonl_file Character string. path output file output_format \"jsonl\". Required case, otherwise ignored. full JSON response appended single line.","code":""},{"path":"https://jdollman.github.io/callm/reference/single_turn.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Call Large Language Models (Single Turn) — single_turn","text":"output_format \"text\", returns extracted text content character string (NA extraction fails). output_format \"jsonl\", appends full JSON response specified file returns invisible(extracted_text) (extracted_text attempted text extraction, possibly NA). Stops execution error message failure (e.g., missing API key, API error, validation failure, JSON parsing failure). Extraction failures output_format \"jsonl\" produce warning allow function complete file write.","code":""},{"path":"https://jdollman.github.io/callm/reference/single_turn.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Call Large Language Models (Single Turn) — single_turn","text":"","code":"if (FALSE) { # \\dontrun{ # Ensure API keys are set as environment variables: # Sys.setenv(GOOGLE_API_KEY = \"YOUR_GOOGLE_KEY\") # Sys.setenv(ANTHROPIC_API_KEY = \"YOUR_ANTHROPIC_KEY\") # Sys.setenv(OPENAI_API_KEY = \"YOUR_OPENAI_KEY\")  # --- Text Output Examples ---  # Google (default org) response_text_google <- single_turn(user = \"Explain the concept of recursion simply.\") print(response_text_google)  # Anthropic response_text_anthropic <- single_turn(   user = \"Write a short poem about R programming.\",   org = \"anthropic\",   model = \"claude-3-sonnet-20240229\" # Use Sonnet instead of default Haiku ) print(response_text_anthropic)  # OpenAI with system message response_text_openai <- single_turn(   user = \"Why is the sky blue?\",   system = \"Explain like I'm five years old.\",   org = \"openai\",   model = \"gpt-4o\",   temperature = 0.7 ) print(response_text_openai)  # --- JSONL Output Example ---  tmp_file <- tempfile(fileext = \".jsonl\") print(paste(\"Writing JSONL output to:\", tmp_file))  # The return value is now the text (invisibly) invisible_text_google <- single_turn(   user = \"What is the capital of France?\",   org = \"google\",   output_format = \"jsonl\",   jsonl_file = tmp_file ) # Can capture it if needed: print(paste(\"Invisible text from Google call:\", invisible_text_google))  invisible_text_openai <- single_turn(   user = \"What are the main benefits of using version control?\",   system = \"You are a helpful software development assistant.\",   org = \"openai\",   output_format = \"jsonl\",   jsonl_file = tmp_file ) print(paste(\"Invisible text from OpenAI call:\", invisible_text_openai))  # Read the results from the file results <- readLines(tmp_file) cat(\"Contents of JSONL file:\\n\") cat(results, sep = \"\\n\")  # Clean up the temporary file unlink(tmp_file) } # }"},{"path":"https://jdollman.github.io/callm/reference/single_turns.html","id":null,"dir":"Reference","previous_headings":"","what":"Call Large Language Models (Multiple Single Turns) — single_turns","title":"Call Large Language Models (Multiple Single Turns) — single_turns","text":"Sequentially sends multiple independent user messages (potentially system message) specified LLM provider model. Handles logging raw responses offers experimental batch processing via OpenAI Batch API.","code":""},{"path":"https://jdollman.github.io/callm/reference/single_turns.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Call Large Language Models (Multiple Single Turns) — single_turns","text":"","code":"single_turns(   user_msgs,   system_msgs = NULL,   org = c(\"google\", \"anthropic\", \"openai\"),   model = NULL,   temperature = 0,   max_tokens = 1024L,   timeout = 60,   log_jsonl = TRUE,   jsonl_file = NULL,   batch = FALSE,   ... )"},{"path":"https://jdollman.github.io/callm/reference/single_turns.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Call Large Language Models (Multiple Single Turns) — single_turns","text":"user_msgs Character vector. vector user messages/prompts. Required. system_msgs Character string vector, NULL. System message(s). See Details behavior. Default NULL. org Character vector. LLM provider. Defaults \"google\". Handles partial matching. Allowed values: \"google\", \"anthropic\", \"openai\". model Character string. specific model ID. Defaults NULL, triggering provider-specific defaults (see single_turn). temperature Numeric. Sampling temperature (>= 0). Default 0.0. max_tokens Integer. Maximum tokens per response. Default 1024L. timeout Numeric. Request timeout per call sequential mode, batch creation call batch mode. Default 60. log_jsonl Logical. full JSON response successful call appended JSONL file? Default TRUE. jsonl_file Character string NULL. Path JSONL file logging. NULL (default) log_jsonl TRUE, filename generated automatically (e.g., \"llm_calls_YYYYMMDD_HHMMSS.jsonl\"). Ignored log_jsonl FALSE batch = TRUE. batch Logical. Use batch processing (currently OpenAI )? Default FALSE. ... Additional arguments passed single_turn (sequential mode) potentially used batch preparation (currently unused).","code":""},{"path":"https://jdollman.github.io/callm/reference/single_turns.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Call Large Language Models (Multiple Single Turns) — single_turns","text":"batch = FALSE: character vector length user_msgs, containing extracted text responses. NA_character_ indicates error occurred specific prompt API call processing. batch = TRUE org = \"openai\": OpenAI batch job ID (character string). batch = TRUE org != \"openai\": Stops error message.","code":""},{"path":"https://jdollman.github.io/callm/reference/single_turns.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Call Large Language Models (Multiple Single Turns) — single_turns","text":"function iterates provided user_msgs prompts. prompt, determines corresponding system message based system_msgs argument calls underlying single_turn function. System Prompt Handling: system_msgs NULL (default), system message used prompt. system_msgs single character string, string used system message user prompts. system_msgs character vector, must length user_msgs. system_msgs[] used user_msgs[]. Output Logging: default (log_jsonl = TRUE), full JSON response API successful call appended JSONL file. jsonl_file provided, filename automatically generated based timestamp. path JSONL file printed console logging active. primary return value (batch = FALSE) character vector containing extracted text responses, NA_character_ indicating failures specific prompts. Batch Processing (Experimental - OpenAI ): batch = TRUE org = \"openai\", function : Prepare JSONL file suitable OpenAI Batch API. Upload file OpenAI. Create batch processing job. Print messages indicating uploaded file ID created batch job ID. Return batch job ID character string. Note: actual results batch job retrieved function; need check job status download results separately using batch ID (functionality potentially added package later). Batch processing providers yet implemented.","code":""},{"path":"https://jdollman.github.io/callm/reference/single_turns.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Call Large Language Models (Multiple Single Turns) — single_turns","text":"","code":"if (FALSE) { # \\dontrun{ # Ensure API keys are set # Sys.setenv(GOOGLE_API_KEY = \"YOUR_GOOGLE_KEY\") # Sys.setenv(OPENAI_API_KEY = \"YOUR_OPENAI_KEY\")  prompts <- c(\"What is R?\", \"Explain dplyr::mutate\", \"Why use version control?\") system_general <- \"You are a helpful R programming assistant.\"  # --- Sequential Execution (Default) ---  # Using Google with default logging responses_google <- single_turns(user_msgs = prompts, org = \"google\") print(responses_google)  # Using OpenAI with a single system prompt and disabling logging responses_openai <- single_turns(   user_msgs = prompts,   system_msgs = system_general,   org = \"openai\",   model = \"gpt-4o-mini\",   log_jsonl = FALSE ) print(responses_openai)  # Using specific system prompts per user prompt specific_system_msgs <- c(\"Explain like I'm 5\", \"Explain for data analyst\", NA) # NA -> NULL system responses_mixed_system_msgs <- single_turns(   user_msgs = prompts,   system_msgs = specific_system_msgs,   org = \"openai\" ) print(responses_mixed_system_msgs)  # Specify a custom JSONL file location my_log <- tempfile(fileext = \".jsonl\") responses_custom_log <- single_turns(   user_msgs = prompts[1:2],   org = \"google\",   jsonl_file = my_log ) print(readLines(my_log)) unlink(my_log)  # --- Batch Execution (OpenAI Only Example) ---  # Note: This only *creates* the batch job. Results must be fetched later. prompts_for_batch <- paste(\"Translate to French:\", c(\"Hello\", \"Goodbye\", \"Thank you\"))  batch_id <- single_turns(   user_msgs = prompts_for_batch,   org = \"openai\",   model = \"gpt-4o-mini\", # Ensure model supports batch if needed   batch = TRUE )  if (!is.null(batch_id)) {  print(paste(\"OpenAI Batch job created with ID:\", batch_id))  # You would later use batch_id to check status and get results }  # Example of trying batch with non-OpenAI provider (will stop) tryCatch({   single_turns(user_msgs = prompts, org = \"google\", batch = TRUE) }, error = function(e) {   print(paste(\"Caught expected error:\", e$message)) })  } # }"},{"path":"https://jdollman.github.io/callm/reference/sitrep.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Situation Report on LLM Provider Prerequisites — sitrep","title":"Get Situation Report on LLM Provider Prerequisites — sitrep","text":"Verifies API key setup via environment variables checks basic API connectivity specified LLM providers (Google, OpenAI, Anthropic). Provides informative messages returns results invisibly.","code":""},{"path":"https://jdollman.github.io/callm/reference/sitrep.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Situation Report on LLM Provider Prerequisites — sitrep","text":"","code":"sitrep(orgs = c(\"google\", \"openai\", \"anthropic\"), verbose = TRUE)"},{"path":"https://jdollman.github.io/callm/reference/sitrep.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Situation Report on LLM Provider Prerequisites — sitrep","text":"orgs character vector specifying providers check. Supported values \"google\", \"openai\", \"anthropic\". Defaults checking supported providers. verbose Logical. TRUE (default), prints detailed status messages, including suggestions, console.","code":""},{"path":"https://jdollman.github.io/callm/reference/sitrep.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get Situation Report on LLM Provider Prerequisites — sitrep","text":"(Invisibly) named list summarizing check results requested provider. element list corresponds provider contains another list two logical elements: key_set TRUE expected environment variable API key set non-empty, FALSE otherwise. api_ok TRUE basic API call succeeded (typically HTTP 2xx status), FALSE API call failed (network error, auth error, timeout, etc.), NA API key set (thus check skipped).","code":""},{"path":"https://jdollman.github.io/callm/reference/sitrep.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get Situation Report on LLM Provider Prerequisites — sitrep","text":"","code":"if (FALSE) { # \\dontrun{   # Run checks interactively (messages print, result list doesn't auto-print)   sitrep()    # Capture the results programmatically   check_results <- sitrep(verbose = FALSE) # Quieter run   if (!isTRUE(check_results$openai$key_set)) {     message(\"OpenAI key not found. See ?sitrep for setup help.\")   }   print(check_results) # Explicitly print the captured list    # Check only Anthropic   anthropic_status <- sitrep(\"anthropic\") } # }"},{"path":"https://jdollman.github.io/callm/reference/workspace_batch.html","id":null,"dir":"Reference","previous_headings":"","what":"Fetch, Process, and Load OpenAI Batch Job Results into Workspace — workspace_batch","title":"Fetch, Process, and Load OpenAI Batch Job Results into Workspace — workspace_batch","text":"Retrieves results completed OpenAI batch job, processes according original API endpoint called (e.g., chat completions, embeddings), returns R object.","code":""},{"path":"https://jdollman.github.io/callm/reference/workspace_batch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fetch, Process, and Load OpenAI Batch Job Results into Workspace — workspace_batch","text":"","code":"workspace_batch(batch_id, timeout = 120)"},{"path":"https://jdollman.github.io/callm/reference/workspace_batch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fetch, Process, and Load OpenAI Batch Job Results into Workspace — workspace_batch","text":"batch_id Character string. ID OpenAI batch job (e.g., \"batch_abc123\"). Required. timeout Numeric. Request timeout seconds, applied status check file download requests. Default 120 (allowing time potential file download).","code":""},{"path":"https://jdollman.github.io/callm/reference/workspace_batch.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fetch, Process, and Load OpenAI Batch Job Results into Workspace — workspace_batch","text":"type object returned depends batch job's endpoint: /v1/chat/completions: character vector responses (NA_character_ failures). /v1/embeddings: list element numeric embedding vector (NULL failures). endpoints: list element raw parsed body response JSONL line (NULL failures). Returns NULL batch job yet completed, critical API calls fail, essential information (like output file ID endpoint) missing.","code":""},{"path":"https://jdollman.github.io/callm/reference/workspace_batch.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Fetch, Process, and Load OpenAI Batch Job Results into Workspace — workspace_batch","text":"function checks batch job status. \"completed\", stops. completed, downloads corresponding output file OpenAI. function parses output JSONL file. Crucially, inspects endpoint field recorded batch job status (e.g., /v1/chat/completions /v1/embeddings) determine process body within successful response line. /v1/chat/completions: Extracts text content. /v1/embeddings: Extracts numeric embedding vector. endpoints: Returns raw parsed body (list) issues warning. Failed requests within batch represented NA_character_ (chat) NULL (embeddings types). results ordered according custom_id (e.g., \"request-1\", \"request-2\", ...) used batch creation.","code":""},{"path":"https://jdollman.github.io/callm/reference/workspace_batch.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fetch, Process, and Load OpenAI Batch Job Results into Workspace — workspace_batch","text":"","code":"if (FALSE) { # \\dontrun{ # Ensure API key is set # Sys.setenv(OPENAI_API_KEY = \"YOUR_OPENAI_KEY\")  # --- Example for a completed Chat Completion batch --- completed_chat_batch_id <- \"batch_chat_xxxxxxxx\" # Replace with your actual ID chat_results <- workspace_batch(completed_chat_batch_id) if (!is.null(chat_results)) {   print(\"Fetched Chat Batch Results:\")   print(chat_results) }  # --- Example for a completed Embeddings batch --- completed_embed_batch_id <- \"batch_embed_yyyyyyyy\" # Replace with your actual ID embedding_results <- workspace_batch(completed_embed_batch_id) if (!is.null(embedding_results)) {   print(\"Fetched Embedding Batch Results:\")   # Print dimensions of the first embedding   if(length(embedding_results) > 0 && is.numeric(embedding_results[[1]])) {      print(paste(\"Dim of first embedding:\", length(embedding_results[[1]])))   } } } # }"},{"path":"https://jdollman.github.io/callm/news/index.html","id":"callm-0009000","dir":"Changelog","previous_headings":"","what":"callm 0.0.0.9000","title":"callm 0.0.0.9000","text":"Checking environment API keys (sitrep) Single-turn chat completions (single_turn) OpenAI, Google, Anthropic. Multiple single-turn chat completions (single_turns) OpenAI, Google, Anthropic. Text embeddings (embed) OpenAI (small/large, dimensions). Initiation via single_turns(batch=TRUE) embed(batch=TRUE). Status checking (check_batch) user-friendly summaries. Result retrieval (workspace_batch) handling different endpoints (chat/embedding). Includes basic non-mocked tests TODOs future mocking.","code":""}]
