% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/polyphony.R
\name{polyphony}
\alias{polyphony}
\title{Send a Prompt Sequentially to Multiple LLM Perspectives (Text Output)}
\usage{
polyphony(
  user,
  perspectives,
  system = NULL,
  max_tokens = 1024L,
  timeout = 60,
  verbose = TRUE,
  error_strategy = c("return_partial", "stop")
)
}
\arguments{
\item{user}{Character string. The user prompt to send to all perspectives. Required.}

\item{perspectives}{A list where each element is itself a list defining an
LLM configuration to query. Each inner list \emph{must} contain:
\itemize{
\item \code{id}: A unique character string identifier for this perspective (used for naming the output).
\item \code{org}: Character string specifying the LLM provider (e.g., "google", "openai", "anthropic"). Passed to \code{single_turn}.
\item \code{model}: Character string specifying the model name for the provider. Passed to \code{single_turn}.
}
Inner lists can \emph{optionally} contain other arguments accepted by
\code{single_turn}, such as \code{temperature}, \code{max_tokens}, \code{timeout}, etc. These
will override the top-level defaults (\code{max_tokens}, \code{timeout}) if provided.
\strong{Note:} Any \code{output_format} or \code{jsonl_file} arguments within a perspective
will be ignored, as \code{polyphony} forces text output.}

\item{system}{Optional character string. A system prompt to be applied
identically to all perspectives. Defaults to \code{NULL}.}

\item{max_tokens}{Integer. The default maximum number of tokens to generate.
This is used if a perspective does not specify its own \code{max_tokens}.
Defaults to \code{1024L}.}

\item{timeout}{Numeric. The default request timeout in seconds. This is used
if a perspective does not specify its own \code{timeout}. Defaults to \code{60}.}

\item{verbose}{Logical. If \code{TRUE} (default), prints status messages indicating
which perspective is currently being processed.}

\item{error_strategy}{Character string defining behavior when one or more
perspectives encounter an error during the API call. Must be one of:
\itemize{
\item \code{"return_partial"} (default): Returns a list containing text results for
successful perspectives and error objects for failed ones.
\item \code{"stop"}: If any perspective fails, the entire function stops and
throws an error, summarizing which perspectives failed.
}}
}
\value{
A named list. The names are the \code{id}s from the \code{perspectives} input list.
The values are:
\itemize{
\item If the call for that perspective was successful: The character string
containing the LLM's text response (obtained via \code{single_turn(..., output_format = "text")}).
\item If the call failed and \code{error_strategy = "return_partial"}: The
\code{error} object captured during the failed \code{single_turn} call.
}
}
\description{
Executes a single user prompt sequentially against multiple Large Language Model
(LLM) configurations defined in the \code{perspectives} list. It leverages the
core \code{single_turn} function for individual API interactions, collecting only
the text responses. Provides verbose output on progress.
}
\details{
This function acts as a multiplexer, sending the same query sequentially to
different models/providers and collecting their textual responses. It relies
on the underlying \code{single_turn} function for handling the specifics of each
provider's API, authentication, and response parsing, ensuring that
\code{output_format} is always set to \code{"text"}.

Authentication relies on API keys being available as environment variables,
as handled by \code{single_turn} (e.g., \code{GOOGLE_API_KEY}, \code{OPENAI_API_KEY},
\code{ANTHROPIC_API_KEY}).
}
\examples{
\dontrun{
# Make sure the single_turn function is available (e.g., via devtools::load_all())
# Ensure API keys are set as environment variables:
# Sys.setenv(GOOGLE_API_KEY = "YOUR_GOOGLE_KEY")
# Sys.setenv(ANTHROPIC_API_KEY = "YOUR_ANTHROPIC_KEY")
# Sys.setenv(OPENAI_API_KEY = "YOUR_OPENAI_KEY")

# Define perspectives
perspectives_list <- list(
  list(id = "gpt4o_mini", org = "openai", model = "gpt-4o-mini", temperature = 0.5),
  list(id = "claude3h", org = "anthro", model = "claude-3-haiku-20240307"), # Partial org match
  list(id = "gemini_flash", org = "google", model = "gemini-1.5-flash-latest", max_tokens = 500)
)

# --- Sequential Execution with Verbose Output (Default) ---
results_seq_verbose <- polyphony(
  user = "Explain the concept of 'polyphony' in music.",
  perspectives = perspectives_list,
  system = "You are a helpful assistant."
)
print(results_seq_verbose)

# --- Sequential Execution without Verbose Output ---
results_seq_quiet <- polyphony(
  user = "Explain the concept of 'polyphony' in music.",
  perspectives = perspectives_list,
  system = "You are a helpful assistant.",
  verbose = FALSE
)
print(results_seq_quiet)

# --- Example with error handling ---
perspectives_with_error <- list(
  list(id = "gpt4o_mini_ok", org = "openai", model = "gpt-4o-mini"),
  list(id = "invalid_model", org = "openai", model = "non-existent-model-123")
)

# Stop on error
tryCatch({
  polyphony(
    user = "Hello",
    perspectives = perspectives_with_error,
    error_strategy = "stop"
  )
}, error = function(e) {
  message("Caught expected error: ", conditionMessage(e))
})

# Return partial results (verbose output will show processing for both)
results_partial <- polyphony(
  user = "Hello",
  perspectives = perspectives_with_error,
  error_strategy = "return_partial"
)
print(results_partial)
# Check which ones failed
print(sapply(results_partial, inherits, "error"))

}
}
