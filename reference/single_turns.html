<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>Call Large Language Models (Multiple Single Turns) — single_turns • callm</title><!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png"><link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png"><link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../apple-touch-icon.png"><link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../apple-touch-icon-120x120.png"><link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../apple-touch-icon-76x76.png"><link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../apple-touch-icon-60x60.png"><script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet"><script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet"><link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet"><script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Call Large Language Models (Multiple Single Turns) — single_turns"><meta name="description" content="Sequentially sends multiple independent user messages (each potentially with a
system message) to a specified LLM provider and model. Handles logging of
raw responses and offers experimental batch processing via the OpenAI Batch API."><meta property="og:description" content="Sequentially sends multiple independent user messages (each potentially with a
system message) to a specified LLM provider and model. Handles logging of
raw responses and offers experimental batch processing via the OpenAI Batch API."><meta property="og:image" content="/logo.png"></head><body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">callm</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.0.0.9000</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto"><li class="active nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="nav-item"><a class="nav-link" href="../news/index.html">Changelog</a></li>
      </ul><ul class="navbar-nav"><li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json"></form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/jdollman/callm/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul></div>


  </div>
</nav><div class="container template-reference-topic">
<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="../logo.png" class="logo" alt=""><h1>Call Large Language Models (Multiple Single Turns)</h1>
      <small class="dont-index">Source: <a href="https://github.com/jdollman/callm/blob/master/R/single_turns.R" class="external-link"><code>R/single_turns.R</code></a></small>
      <div class="d-none name"><code>single_turns.Rd</code></div>
    </div>

    <div class="ref-description section level2">
    <p>Sequentially sends multiple independent user messages (each potentially with a
system message) to a specified LLM provider and model. Handles logging of
raw responses and offers experimental batch processing via the OpenAI Batch API.</p>
    </div>

    <div class="section level2">
    <h2 id="ref-usage">Usage<a class="anchor" aria-label="anchor" href="#ref-usage"></a></h2>
    <div class="sourceCode"><pre class="sourceCode r"><code><span><span class="fu">single_turns</span><span class="op">(</span></span>
<span>  <span class="va">user_msgs</span>,</span>
<span>  system_msgs <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  org <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"google"</span>, <span class="st">"anthropic"</span>, <span class="st">"openai"</span><span class="op">)</span>,</span>
<span>  model <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  temperature <span class="op">=</span> <span class="fl">0</span>,</span>
<span>  max_tokens <span class="op">=</span> <span class="fl">1024L</span>,</span>
<span>  timeout <span class="op">=</span> <span class="fl">60</span>,</span>
<span>  log_jsonl <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>  jsonl_file <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  batch <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>  <span class="va">...</span></span>
<span><span class="op">)</span></span></code></pre></div>
    </div>

    <div class="section level2">
    <h2 id="arguments">Arguments<a class="anchor" aria-label="anchor" href="#arguments"></a></h2>


<dl><dt id="arg-user-msgs">user_msgs<a class="anchor" aria-label="anchor" href="#arg-user-msgs"></a></dt>
<dd><p>Character vector. A vector of user messages/prompts. Required.</p></dd>


<dt id="arg-system-msgs">system_msgs<a class="anchor" aria-label="anchor" href="#arg-system-msgs"></a></dt>
<dd><p>Character string or vector, or NULL. System message(s).
See Details for behavior. Default is <code>NULL</code>.</p></dd>


<dt id="arg-org">org<a class="anchor" aria-label="anchor" href="#arg-org"></a></dt>
<dd><p>Character vector. The LLM provider. Defaults to "google".
Handles partial matching. Allowed values: "google", "anthropic", "openai".</p></dd>


<dt id="arg-model">model<a class="anchor" aria-label="anchor" href="#arg-model"></a></dt>
<dd><p>Character string. The specific model ID. Defaults to <code>NULL</code>,
triggering provider-specific defaults (see <code>single_turn</code>).</p></dd>


<dt id="arg-temperature">temperature<a class="anchor" aria-label="anchor" href="#arg-temperature"></a></dt>
<dd><p>Numeric. Sampling temperature (&gt;= 0). Default is 0.0.</p></dd>


<dt id="arg-max-tokens">max_tokens<a class="anchor" aria-label="anchor" href="#arg-max-tokens"></a></dt>
<dd><p>Integer. Maximum tokens per response. Default is 1024L.</p></dd>


<dt id="arg-timeout">timeout<a class="anchor" aria-label="anchor" href="#arg-timeout"></a></dt>
<dd><p>Numeric. Request timeout <em>per call</em> in sequential mode, or
for the batch <em>creation</em> call in batch mode. Default is 60.</p></dd>


<dt id="arg-log-jsonl">log_jsonl<a class="anchor" aria-label="anchor" href="#arg-log-jsonl"></a></dt>
<dd><p>Logical. Should the full JSON response for each successful
call be appended to a JSONL file? Default is <code>TRUE</code>.</p></dd>


<dt id="arg-jsonl-file">jsonl_file<a class="anchor" aria-label="anchor" href="#arg-jsonl-file"></a></dt>
<dd><p>Character string or <code>NULL</code>. Path to the JSONL file for logging.
If <code>NULL</code> (default) and <code>log_jsonl</code> is <code>TRUE</code>, a filename is generated
automatically (e.g., "llm_calls_YYYYMMDD_HHMMSS.jsonl"). Ignored if
<code>log_jsonl</code> is <code>FALSE</code> or if <code>batch = TRUE</code>.</p></dd>


<dt id="arg-batch">batch<a class="anchor" aria-label="anchor" href="#arg-batch"></a></dt>
<dd><p>Logical. Use batch processing (currently OpenAI only)? Default <code>FALSE</code>.</p></dd>


<dt id="arg--">...<a class="anchor" aria-label="anchor" href="#arg--"></a></dt>
<dd><p>Additional arguments to be passed to <code>single_turn</code> (in sequential mode)
or potentially used in batch preparation (currently unused).</p></dd>

</dl></div>
    <div class="section level2">
    <h2 id="value">Value<a class="anchor" aria-label="anchor" href="#value"></a></h2>

<ul><li><p>If <code>batch = FALSE</code>: A character vector of the same length as <code>user_msgs</code>,
containing the extracted text responses. <code>NA_character_</code> indicates an error
occurred for that specific prompt during the API call or processing.</p></li>
<li><p>If <code>batch = TRUE</code> and <code>org = "openai"</code>: The OpenAI batch job ID (character string).</p></li>
<li><p>If <code>batch = TRUE</code> and <code>org != "openai"</code>: Stops with an error message.</p></li>
</ul></div>
    <div class="section level2">
    <h2 id="details">Details<a class="anchor" aria-label="anchor" href="#details"></a></h2>
    <p>This function iterates through the provided <code>user_msgs</code> prompts. For each prompt,
it determines the corresponding system message based on the <code>system_msgs</code> argument
and calls the underlying <code>single_turn</code> function.</p>
<p><strong>System Prompt Handling:</strong></p><ul><li><p>If <code>system_msgs</code> is <code>NULL</code> (default), no system message is used for any prompt.</p></li>
<li><p>If <code>system_msgs</code> is a single character string, that string is used as the system
message for <em>all</em> user prompts.</p></li>
<li><p>If <code>system_msgs</code> is a character vector, it <em>must</em> be the same length as <code>user_msgs</code>.
<code>system_msgs[i]</code> will be used with <code>user_msgs[i]</code>.</p></li>
</ul><p><strong>Output and Logging:</strong>
By default (<code>log_jsonl = TRUE</code>), the full JSON response from the API for each
successful call is appended to a JSONL file. If <code>jsonl_file</code> is not provided,
a filename is automatically generated based on the timestamp. The path to the
JSONL file is printed to the console when logging is active. The primary return
value (when <code>batch = FALSE</code>) is a character vector containing the extracted text
responses, with <code>NA_character_</code> indicating failures for specific prompts.</p>
<p><strong>Batch Processing (Experimental - OpenAI Only):</strong>
If <code>batch = TRUE</code> and <code>org = "openai"</code>, the function will:</p><ol><li><p>Prepare a JSONL file suitable for the OpenAI Batch API.</p></li>
<li><p>Upload this file to OpenAI.</p></li>
<li><p>Create a batch processing job.</p></li>
<li><p>Print messages indicating the uploaded file ID and the created batch job ID.</p></li>
<li><p><strong>Return the batch job ID</strong> as a character string.
Note: The actual results of the batch job are <em>not</em> retrieved by this function;
you will need to check the job status and download the results separately
using the batch ID (functionality potentially added to this package later).
Batch processing for other providers is not yet implemented.</p></li>
</ol></div>

    <div class="section level2">
    <h2 id="ref-examples">Examples<a class="anchor" aria-label="anchor" href="#ref-examples"></a></h2>
    <div class="sourceCode"><pre class="sourceCode r"><code><span class="r-in"><span><span class="kw">if</span> <span class="op">(</span><span class="cn">FALSE</span><span class="op">)</span> <span class="op">{</span> <span class="co"># \dontrun{</span></span></span>
<span class="r-in"><span><span class="co"># Ensure API keys are set</span></span></span>
<span class="r-in"><span><span class="co"># Sys.setenv(GOOGLE_API_KEY = "YOUR_GOOGLE_KEY")</span></span></span>
<span class="r-in"><span><span class="co"># Sys.setenv(OPENAI_API_KEY = "YOUR_OPENAI_KEY")</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="va">prompts</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"What is R?"</span>, <span class="st">"Explain dplyr::mutate"</span>, <span class="st">"Why use version control?"</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="va">system_general</span> <span class="op">&lt;-</span> <span class="st">"You are a helpful R programming assistant."</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># --- Sequential Execution (Default) ---</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># Using Google with default logging</span></span></span>
<span class="r-in"><span><span class="va">responses_google</span> <span class="op">&lt;-</span> <span class="fu">single_turns</span><span class="op">(</span>user_msgs <span class="op">=</span> <span class="va">prompts</span>, org <span class="op">=</span> <span class="st">"google"</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">responses_google</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># Using OpenAI with a single system prompt and disabling logging</span></span></span>
<span class="r-in"><span><span class="va">responses_openai</span> <span class="op">&lt;-</span> <span class="fu">single_turns</span><span class="op">(</span></span></span>
<span class="r-in"><span>  user_msgs <span class="op">=</span> <span class="va">prompts</span>,</span></span>
<span class="r-in"><span>  system_msgs <span class="op">=</span> <span class="va">system_general</span>,</span></span>
<span class="r-in"><span>  org <span class="op">=</span> <span class="st">"openai"</span>,</span></span>
<span class="r-in"><span>  model <span class="op">=</span> <span class="st">"gpt-4o-mini"</span>,</span></span>
<span class="r-in"><span>  log_jsonl <span class="op">=</span> <span class="cn">FALSE</span></span></span>
<span class="r-in"><span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">responses_openai</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># Using specific system prompts per user prompt</span></span></span>
<span class="r-in"><span><span class="va">specific_system_msgs</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"Explain like I'm 5"</span>, <span class="st">"Explain for data analyst"</span>, <span class="cn">NA</span><span class="op">)</span> <span class="co"># NA -&gt; NULL system</span></span></span>
<span class="r-in"><span><span class="va">responses_mixed_system_msgs</span> <span class="op">&lt;-</span> <span class="fu">single_turns</span><span class="op">(</span></span></span>
<span class="r-in"><span>  user_msgs <span class="op">=</span> <span class="va">prompts</span>,</span></span>
<span class="r-in"><span>  system_msgs <span class="op">=</span> <span class="va">specific_system_msgs</span>,</span></span>
<span class="r-in"><span>  org <span class="op">=</span> <span class="st">"openai"</span></span></span>
<span class="r-in"><span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">responses_mixed_system_msgs</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># Specify a custom JSONL file location</span></span></span>
<span class="r-in"><span><span class="va">my_log</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/tempfile.html" class="external-link">tempfile</a></span><span class="op">(</span>fileext <span class="op">=</span> <span class="st">".jsonl"</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="va">responses_custom_log</span> <span class="op">&lt;-</span> <span class="fu">single_turns</span><span class="op">(</span></span></span>
<span class="r-in"><span>  user_msgs <span class="op">=</span> <span class="va">prompts</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">2</span><span class="op">]</span>,</span></span>
<span class="r-in"><span>  org <span class="op">=</span> <span class="st">"google"</span>,</span></span>
<span class="r-in"><span>  jsonl_file <span class="op">=</span> <span class="va">my_log</span></span></span>
<span class="r-in"><span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/readLines.html" class="external-link">readLines</a></span><span class="op">(</span><span class="va">my_log</span><span class="op">)</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="fu"><a href="https://rdrr.io/r/base/unlink.html" class="external-link">unlink</a></span><span class="op">(</span><span class="va">my_log</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># --- Batch Execution (OpenAI Only Example) ---</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># Note: This only *creates* the batch job. Results must be fetched later.</span></span></span>
<span class="r-in"><span><span class="va">prompts_for_batch</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/paste.html" class="external-link">paste</a></span><span class="op">(</span><span class="st">"Translate to French:"</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"Hello"</span>, <span class="st">"Goodbye"</span>, <span class="st">"Thank you"</span><span class="op">)</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="va">batch_id</span> <span class="op">&lt;-</span> <span class="fu">single_turns</span><span class="op">(</span></span></span>
<span class="r-in"><span>  user_msgs <span class="op">=</span> <span class="va">prompts_for_batch</span>,</span></span>
<span class="r-in"><span>  org <span class="op">=</span> <span class="st">"openai"</span>,</span></span>
<span class="r-in"><span>  model <span class="op">=</span> <span class="st">"gpt-4o-mini"</span>, <span class="co"># Ensure model supports batch if needed</span></span></span>
<span class="r-in"><span>  batch <span class="op">=</span> <span class="cn">TRUE</span></span></span>
<span class="r-in"><span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="kw">if</span> <span class="op">(</span><span class="op">!</span><span class="fu"><a href="https://rdrr.io/r/base/NULL.html" class="external-link">is.null</a></span><span class="op">(</span><span class="va">batch_id</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span></span>
<span class="r-in"><span> <span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/paste.html" class="external-link">paste</a></span><span class="op">(</span><span class="st">"OpenAI Batch job created with ID:"</span>, <span class="va">batch_id</span><span class="op">)</span><span class="op">)</span></span></span>
<span class="r-in"><span> <span class="co"># You would later use batch_id to check status and get results</span></span></span>
<span class="r-in"><span><span class="op">}</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># Example of trying batch with non-OpenAI provider (will stop)</span></span></span>
<span class="r-in"><span><span class="kw"><a href="https://rdrr.io/r/base/conditions.html" class="external-link">tryCatch</a></span><span class="op">(</span><span class="op">{</span></span></span>
<span class="r-in"><span>  <span class="fu">single_turns</span><span class="op">(</span>user_msgs <span class="op">=</span> <span class="va">prompts</span>, org <span class="op">=</span> <span class="st">"google"</span>, batch <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="op">}</span>, error <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">e</span><span class="op">)</span> <span class="op">{</span></span></span>
<span class="r-in"><span>  <span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/paste.html" class="external-link">paste</a></span><span class="op">(</span><span class="st">"Caught expected error:"</span>, <span class="va">e</span><span class="op">$</span><span class="va">message</span><span class="op">)</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="op">}</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="op">}</span> <span class="co"># }</span></span></span>
</code></pre></div>
    </div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside></div>


    <footer><div class="pkgdown-footer-left">
  <p>Developed by Justin Dollman.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.1.</p>
</div>

    </footer></div>





  </body></html>

